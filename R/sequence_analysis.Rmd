---
title: "Hemiptera metabarcoding analysis"
author: "Alexander Piper"
date: "10/04/2018"
output: html_document
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE)
opts_knit$set(root.dir = 'C:/Users/ap0y/Dropbox/Work Projects/PHD/Metabarcoding/drosophila_metabarcoding')
opts_chunk$set(dev = 'pdf')
```

##Load all requried packages
```{r load required packages}
sapply(c("dada2", "phyloseq","ggplot2","ips", "DECIPHER", "fastqcr" ), require, character.only = TRUE)
sapply(c("phyloseq", "data.table", "ggplot2", "tidyverse","Biostrings","ShortRead"), require, character.only = TRUE)

###Track reads throughout whole process
  # set a little function
getN <- function(x) sum(getUniques(x))


```

First step of this workflow will be print quality profiles of the runs

PROBLEM: Fastqcr requires mac or unix system
```{R}
runs <- dir("data/", pattern="run")

qc = data.frame

for (i in seq(along=runs)){
fastqc(fq.dir = runs[i], # FASTQ files directory
       qc.dir = paste0(runs[i],"/FASTQC"), # Results direcory
       threads = 4                    # Number of threads
       )
  
 qc[i] <- qc_aggregate(qc.dir) 
}

summary(qc)

qc_fail <- qc %>%
  select(sample, module, status) %>%    
  filter(status %in% c("WARN", "FAIL")) %>%
  arrange(sample)


##Multisample report 
qc_report(qc.dir, result.file = "output/QC",
          experiment = "Drosophila metabarcoding")

##Plot samples

#loop For each run, do a big combined plot pdf for each sample
qc_plot(qc, "Per base sequence quality")

qc_plot(qc, "Per sequence quality scores")

qc_plot(qc, "Per base sequence content")

qc_plot(qc, "Sequence duplication levels")


```

For primer trimming we weill probably need an adapter trimming s
```{R trim primers}
runs <- dir("data/", pattern="run")

##Set primer sequences to search for
FWD_COI <- "ATTGGWGGWTTYGGAAAYTG"  ## Replace with used sequences
REV_COI <- "TATRAARTTRATWGCTCCTA"  

##Get all orientation of primer sequences
allOrients <- function(primer) {
    # Create all orientations of the input sequence
    require(Biostrings)
    dna <- DNAString(primer)  # The Biostrings works w/ DNAString objects rather than character vectors
    orients <- c(Forward = dna, Complement = complement(dna), Reverse = reverse(dna), 
        RevComp = reverseComplement(dna))
    return(sapply(orients, toString))  # Convert back to character vector
}
FWD_COI.orients <- allOrients(FWD_COI)
REV_COI.orients <- allOrients(REV_COI)


#The presence of ambiguous bases (Ns) in the sequencing reads makes accurate mapping of short primer sequences difficult. Next we are going to “pre-filter” the sequences just to remove those with Ns, but perform no other filtering.

##Loop across run files to remove N's
##Try get rid of this
for (i in seq(along=runs)){
path <- paste0("data/",runs[i]) # CHANGE ME to the directory containing your demultiplexed forward-read fastq files
fnFs <- sort(list.files(path, pattern = "R1_001.fastq.gz", full.names = TRUE))
fnRs <- sort(list.files(path, pattern = "R2_001.fastq.gz", full.names = TRUE))

fnFs.filtN <- file.path(path, "filtN", basename(fnFs)) # Put N-filterd files in filtN/ subdirectory
fnRs.filtN <- file.path(path, "filtN", basename(fnRs))
filterAndTrim(fnFs, fnFs.filtN, fnRs, fnRs.filtN, maxN = 2, multithread = TRUE) ##This was orignally trimming maxN = 0 with no trimleft
message(paste0("all N bases removed from ", runs[i]))
}

##Next step is to use cutadapt to remove the primers
##Count the number of times the primers appear in the forward and reverse read, while considering all possible primer orientations
primerHits <- function(primer, fn) {
    # Counts number of reads in which the primer is found
    nhits <- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)
    return(sum(nhits > 0))
}

for (i in seq(along=runs)){
path <- paste0("data/",runs[i]) # CHANGE ME to the directory containing your demultiplexed forward-read fastq files
fnFs <- sort(list.files(path, pattern = "R1_001.fastq.gz", full.names = TRUE))
fnRs <- sort(list.files(path, pattern = "R2_001.fastq.gz", full.names = TRUE))
fnFs.filtN <- file.path(path, basename(fnFs)) # Put N-filterd files in filtN/ subdirectory
fnRs.filtN <- file.path(path, basename(fnRs))

##Sanity check - How many hits in sample 1
hits <- rbind(
    FWD_COI.ForwardReads = sapply(FWD_COI.orients, primerHits, fn = fnFs.filtN[[1]]), 
    FWD_COI.ReverseReads = sapply(FWD_COI.orients, primerHits, fn = fnRs.filtN[[1]]), 
    REV_COI.ForwardReads = sapply(REV_COI.orients, primerHits, fn = fnFs.filtN[[1]]), 
    REV_COI.ReverseReads = sapply(REV_COI.orients, primerHits, fn = fnRs.filtN[[1]]))

message(paste0("Sanity check - How many hits in sample 1 of ",runs[i]), " before primer trimming")
print(hits)

cutadapt <- "C:/Users/ap0y/AppData/Roaming/Python/Python36/Scripts/cutadapt" # CHANGE ME to the cutadapt path on your machine
system2(cutadapt, args = "--version") # Run shell commands from R

path.cut <- file.path(path, "cutadapt")
if(!dir.exists(path.cut)) dir.create(path.cut)
fnFs.cut <- file.path(path.cut, basename(fnFs))
fnRs.cut <- file.path(path.cut, basename(fnRs))

FWD_COI.RC <- dada2:::rc(FWD_COI)
REV_COI.RC <- dada2:::rc(REV_COI)

# Trim FWD and the reverse-complement of REV off of R1 (forward reads)
R1.flags <- paste("-g", FWD_COI,"-g",FWD_18S,"-g",FWD_12S, "-a", REV_COI.RC, "-a", REV_18S.RC, "-a", REV_12S.RC) 
# Trim REV and the reverse-complement of FWD off of R2 (reverse reads)
R2.flags <- paste("-G", REV_COI,"-G", REV_18S, "-G", REV_12S, "-A", FWD_COI.RC, "-A", FWD_18S.RC, "-A", FWD_12S.RC ) 
# Run Cutadapt
for(i in seq_along(fnFs)) {
  system2(cutadapt, args = c(R1.flags, R2.flags, "-n", 2, # -n 2 required to remove FWD and REV from reads
                             "-o", fnFs.cut[i], "-p", fnRs.cut[i], # output files
                             fnFs.filtN[i], fnRs.filtN[i])) # input files
  
  
##Post cut Sanity check
cut_hits <- rbind(
    FWD_COI.ForwardReads = sapply(FWD_COI.orients, primerHits, fn = fnFs.filtN[[1]]), 
    FWD_COI.ReverseReads = sapply(FWD_COI.orients, primerHits, fn = fnRs.filtN[[1]]), 
    REV_COI.ForwardReads = sapply(REV_COI.orients, primerHits, fn = fnFs.filtN[[1]]), 
    REV_COI.ReverseReads = sapply(REV_COI.orients, primerHits, fn = fnRs.filtN[[1]]))

message(paste0("Sanity check - How many hits in sample 1 of ",runs[i], " after primer trimming"))
print(cut_hits)
}
}
```


Following filtering for the runs, move files into forward and rev folders


##Plot error on reads
```{r}
runs <- dir("data/", pattern="run")

for (i in seq(along=runs)){
path <- paste0("data/",runs[i]) # CHANGE ME to the directory containing your demultiplexed forward-read fastq files

fastqFs <- sort(list.files(path, pattern="R1_001.trimmed.fastq.gz", full.names = TRUE))
fastqRs <- sort(list.files(path, pattern="R2_001.trimmed.fastq.gz", full.names = TRUE))
print(plotQualityProfile(fastqFs[1:4]) + ggtitle(paste0(runs[i]," Forward Reads")))
print(plotQualityProfile(fastqRs[1:4]) + ggtitle(paste0(runs[i]," Reverse Reads")))
}
```


```{r filter}

##Note - for filtering stage, these parameters may not be optimal for each run or dataset, use the previous plotting step to inform this
runs <- dir("data/", pattern="run")
filtered_out <- list()

for (i in seq(along=runs)){
path <- paste0("data/",runs[i]) # CHANGE ME to the directory containing your demultiplexed forward-read fastq files
filtpath <- file.path(path, "filtered") # Filtered forward files go into the path/filtered/ subdirectory

fastqFs <- sort(list.files(path, pattern="R1_001.trimmed.fastq.gz"))
fastqRs <- sort(list.files(path, pattern="R2_001.trimmed.fastq.gz"))

if(length(fastqFs) != length(fastqRs)) stop(paste0("Forward and reverse files for ",runs[i]," do not match."))

filtered_out[i] <- (filterAndTrim(fwd=file.path(path, fastqFs), filt=file.path(filtpath, fastqFs),
                            rev=file.path(path, fastqRs), filt.rev=file.path(filtpath, fastqRs), trimLeft = 2,
                            maxEE=c(2,5), truncQ=2, maxN = 2, minLen = 150,
		                        rm.phix=TRUE, compress=TRUE, verbose=TRUE))

}
print(filtered_out)
```

##Post filtering error plotting
```{r}
runs <- dir("data/", pattern="run")

for (i in seq(along=runs)){
path <- paste0("data/",runs[i]) # CHANGE ME to the directory containing your demultiplexed forward-read fastq files

fastqFs <- sort(list.files(path, pattern="R1_001.trimmed.fastq.gz", full.names = TRUE))
fastqRs <- sort(list.files(path, pattern="R2_001.trimmed.fastq.gz", full.names = TRUE))
print(plotQualityProfile(fastqFs[1:4]) + ggtitle(paste0(runs[i]," Forward Reads")))
print(plotQualityProfile(fastqRs[1:4]) + ggtitle(paste0(runs[i]," Reverse Reads")))
}
```



```{r Learn error rates }
##Parse filtered files from each ru

#Run1
filtFs1 <- list.files(filtpathF1, pattern="fastq.gz", full.names = TRUE)
filtRs1 <- list.files(filtpathR1, pattern="fastq.gz", full.names = TRUE)
sample.names1 <- sapply(strsplit(basename(filtFs1), "_"), `[`, 1) # Assumes filename = flowcell_samplename_XXX.fastq.gz
sample.namesR1 <- sapply(strsplit(basename(filtRs1), "_"), `[`, 1) # Assumes filename = flowcell_samplename_XXX.fastq.gz
if(!identical(sample.names1, sample.namesR1)) stop("Forward and reverse files from run1 do not match.")
names(filtFs1) <- sample.names1
names(filtRs1) <- sample.names1

#Run2
filtFs2 <- list.files(filtpathF2, pattern="fastq.gz", full.names = TRUE)
filtRs2 <- list.files(filtpathR2, pattern="fastq.gz", full.names = TRUE)
sample.names2 <- sapply(strsplit(basename(filtFs2), "_"), `[`, 1) # Assumes filename = flowcell_samplename_XXX.fastq.gz
sample.namesR2 <- sapply(strsplit(basename(filtRs2), "_"), `[`, 1) # Assumes filename = flowcell_samplename_XXX.fastq.gz
if(!identical(sample.names2, sample.namesR2)) stop("Forward and reverse files from run2 do not match.")
names(filtFs2) <- sample.names2
names(filtRs2) <- sample.names2

#Run3
filtFs3 <- list.files(filtpathF3, pattern="fastq.gz", full.names = TRUE)
filtRs3 <- list.files(filtpathR3, pattern="fastq.gz", full.names = TRUE)
sample.names3 <- sapply(strsplit(basename(filtFs3), "_"), `[`, 1) # Assumes filename = flowcell_samplename_XXX.fastq.gz
sample.namesR3 <- sapply(strsplit(basename(filtRs3), "_"), `[`, 1) # Assumes filename = flowcell_samplename_XXX.fastq.gz
if(!identical(sample.names3, sample.namesR3)) stop("Forward and reverse files from run3 do not match.")
names(filtFs3) <- sample.names3
names(filtRs3) <- sample.names3

set.seed(100)

# Learn error rates from samples
# nread tells the function how many reads to use in error learning, this can be increased for more accuracy at the expense of runtime

# Learn forward error rates
errF1 <- learnErrors(filtFs1, multithread=TRUE)
errF2 <- learnErrors(filtFs2, multithread=TRUE)
errF3 <- learnErrors(filtFs3, multithread=TRUE)
# Learn reverse error rates
errR1 <- learnErrors(filtRs1, multithread=TRUE)
errR2 <- learnErrors(filtRs2, multithread=TRUE)
errR3 <- learnErrors(filtRs3, multithread=TRUE)

##Print error plots to see how well the algorithm modelled the errors in the different runs
print(plotErrors(errF1, nominalQ=TRUE)+ ggtitle("Run 1 Forward Reads"))
print(plotErrors(errR1, nominalQ=TRUE)+ ggtitle("Run 1 Reverse Reads"))
print(plotErrors(errF2, nominalQ=TRUE)+ ggtitle("Run 2 Forward Reads"))
print(plotErrors(errR2, nominalQ=TRUE)+ ggtitle("Run 2 Reverse Reads"))
print(plotErrors(errF3, nominalQ=TRUE)+ ggtitle("Run 3 Forward Reads"))
print(plotErrors(errR3, nominalQ=TRUE)+ ggtitle("Run 3 Reverse Reads"))


```

```{r Infer variants}
set.seed(100)
# Run 1 Sample inference and merger of paired-end reads 
mergers_run1 <- vector("list", length(sample.names1))
names(mergers_run1) <- sample.names1
for(sam in sample.names1) {
  cat("Processing:", sam, "\n")
    derepF <- derepFastq(filtFs1[[sam]])
    ddF <- dada(derepF, err=errF1, multithread=TRUE)
    derepR <- derepFastq(filtRs1[[sam]])
    ddR <- dada(derepR, err=errR1, multithread=TRUE)
    merger <- mergePairs(ddF, derepF, ddR, derepR)
    mergers_run1[[sam]] <- merger
}
rm(derepF); rm(derepR)
# Construct sequence table
seqtab_run1 <- makeSequenceTable(mergers_run1)
saveRDS(seqtab_run1, "data/filtN/run1_250/seqtab.rds") # CHANGE ME to where you want sequence table saved

# Run 2 Sample inference and merger of paired-end reads
mergers_run2 <- vector("list", length(sample.names2))
names(mergers_run2) <- sample.names2
for(sam in sample.names2) {
  cat("Processing:", sam, "\n")
    derepF <- derepFastq(filtFs2[[sam]])
    ddF <- dada(derepF, err=errF2, multithread=TRUE)
    derepR <- derepFastq(filtRs2[[sam]])
    ddR <- dada(derepR, err=errR2, multithread=TRUE)
    merger <- mergePairs(ddF, derepF, ddR, derepR)
    mergers_run2[[sam]] <- merger
}
rm(derepF); rm(derepR)
# Construct sequence table
seqtab_run2 <- makeSequenceTable(mergers_run2)
saveRDS(seqtab_run2, "data/filtN/run2_mock/seqtab.rds") # CHANGE ME to where you want sequence table saved

# Run 3 Sample inference and merger of paired-end reads
mergers_run3 <- vector("list", length(sample.names3))
names(mergers_run3) <- sample.names3
for(sam in sample.names3) {
  cat("Processing:", sam, "\n")
    derepF <- derepFastq(filtFs3[[sam]])
    ddF <- dada(derepF, err=errF3, multithread=TRUE)
    derepR <- derepFastq(filtRs3[[sam]])
    ddR <- dada(derepR, err=errR3, multithread=TRUE)
    merger <- mergePairs(ddF, derepF, ddR, derepR)
    mergers_run3[[sam]] <- merger
}
rm(derepF); rm(derepR)
# Construct sequence table
seqtab_run3 <- makeSequenceTable(mergers_run3)
saveRDS(seqtab_run3, "data/filtN/run3_trap/seqtab.rds") # CHANGE ME to where you want sequence table saved


```


```{R overview of counts throughout}
  # set a little function
getN <- function(x) sum(getUniques(x))

  # making a little table
summary_tab <- data.frame(row.names=samples, raw_data=,N_removed=,primer_trimmed=, dada2_input=filtered_out[,1], filtered=filtered_out[,2], dada_f=sapply(dada_forward, getN), dada_r=sapply(dada_reverse, getN), merged=sapply(merged_amplicons, getN), nonchim=rowSums(seqtab.nochim), total_perc_reads_lost=round(rowSums(seqtab.nochim)/filtered_out[,1]*100, 1))

summary_tab


```
